"""
Pipeline ETL principal para processamento de dados de pre√ßos de combust√≠veis.
"""
import sys
from enum import Enum
from pathlib import Path
from datetime import datetime
import logging
from typing import Optional, Dict, Any

# Adiciona o diret√≥rio raiz ao PYTHONPATH
root_dir = Path(__file__).resolve().parent.parent.parent.parent
sys.path.append(str(root_dir))

from src.pipeline.kaggle.upstream.extractor import KaggleExtractor
from src.pipeline.kaggle.midstream.transformer import MultidimensionalTransformer
from src.infrastructure.data_lake_manager import DataLakeManager

class PipelineStage(Enum):
    """Enumera√ß√£o das etapas do pipeline"""
    EXTRACT = "extract"
    TRANSFORM = "transform"
    LOAD = "load"
    ALL = "all"

def setup_logger() -> logging.Logger:
    """Configura o logger principal do pipeline"""
    logger = logging.getLogger('ETLPipeline')
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        # Handler para console com formata√ß√£o colorida
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formato do log
        formatter = logging.Formatter('%(asctime)s [%(name)s] %(levelname)s: %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    
    return logger

class FuelPriceETL:
    """Pipeline ETL para processamento de dados de pre√ßos de combust√≠veis"""
    
    def __init__(self, base_path: Optional[Path] = None):
        """
        Inicializa o pipeline ETL.
        
        Args:
            base_path: Caminho base para o data lake. Se n√£o fornecido, usa 'data/'.
        """
        # Configurar logger primeiro
        self.logger = setup_logger()
        self.logger.info("Inicializando pipeline ETL...")
        
        # Encontrar diret√≥rio raiz do projeto
        if base_path:
            self.base_path = Path(base_path)
        else:
            self.base_path = Path(__file__).resolve().parent.parent.parent.parent / 'data'
        
        self.logger.info(f"Diret√≥rio base do data lake: {self.base_path}")
        # Inicializar gerenciador do Data Lake
        self.data_lake = DataLakeManager(str(self.base_path))
        
        # A estrutura √© criada automaticamente no __init__ da DataLakeManager        # Inicializar componentes
        self.logger.info("Inicializando componentes...")
        self.extractor = KaggleExtractor(self.data_lake)
        self.transformer = MultidimensionalTransformer(output_base_path=str(self.base_path / "silver"))
        
        # Armazenar resultados das etapas
        self.results = {
            'extract': None,
            'transform': None,
            'load': None
        }
        
        self.logger.info("Pipeline ETL inicializado com sucesso!")
    
    def extract(self) -> Path:
        """
        Executa a etapa de extra√ß√£o.
        
        Returns:
            Path do arquivo extra√≠do
        """
        self.logger.info("\n=== Iniciando etapa de extra√ß√£o ===")
        try:
            input_file = self.extractor.extract_dataset()
            self.results['extract'] = input_file
            self.logger.info(f"Extra√ß√£o conclu√≠da com sucesso!")
            self.logger.info(f"Arquivo salvo em: {input_file}")
            return input_file
        except Exception as e:
            self.logger.error(f"Erro na extra√ß√£o: {str(e)}")
            raise    
    
    def transform(self, input_file: Optional[Path] = None) -> Dict[str, Any]:
        """
        Executa a etapa de transforma√ß√£o, convertendo os dados para modelo Star Schema.
        
        Args:
            input_file: Caminho opcional para o arquivo de entrada. Se n√£o fornecido, usa o resultado da etapa de extra√ß√£o.
        Returns:
            Dicion√°rio com estat√≠sticas e metadados da transforma√ß√£o        
        Executa a etapa de transforma√ß√£o, convertendo os dados para modelo Star Schema.
        
        Args:
            input_file: Path do arquivo a ser transformado. Se n√£o fornecido, usa o resultado da etapa de extra√ß√£o.
        
        Returns:
            Dicion√°rio com estat√≠sticas e metadados da transforma√ß√£o multidimensional
        """
        self.logger.info("\n=== Iniciando etapa de transforma√ß√£o ===")
        try:            # Usar arquivo fornecido ou resultado da extra√ß√£o
            input_file = input_file or self.results.get('extract')
            if not input_file:
                raise ValueError("Nenhum arquivo de entrada dispon√≠vel para transforma√ß√£o")            # Transformar usando modelo dimensional (Star Schema)
            result = self.transformer.transform_to_star_schema(
                input_file=str(input_file)
            )
            
            self.results['transform'] = result
            
            # Log das estat√≠sticas
            self._log_transformation_stats(result)
            
            return result
        except Exception as e:
            self.logger.error(f"Erro na transforma√ß√£o: {str(e)}")
            raise    
        
    def _log_transformation_stats(self, result: Dict[str, Any]):
        """Registra estat√≠sticas da transforma√ß√£o multidimensional"""
        self.logger.info("\nEstat√≠sticas do processamento Star Schema:")
        
        if result.get('success', False):
            self.logger.info(f"‚úÖ Transforma√ß√£o conclu√≠da com sucesso")
            self.logger.info(f"üìä Registros total: {result.get('records_processed', 'N/A')}")
            self.logger.info(f"‚è±Ô∏è  Tempo de processamento: {result.get('processing_time_seconds', 'N/A'):.2f}s")
            self.logger.info(f"üìÅ Caminho Silver: {result.get('output_path', 'N/A')}")
            
            # Log das dimens√µes
            dims = result.get('dimensions', {})
            self.logger.info("\nüìä Dimens√µes geradas:")
            self.logger.info(f"  - Regi√µes: {dims.get('regiao', 'N/A')}")
            self.logger.info(f"  - Estados: {dims.get('estado', 'N/A')}")
            self.logger.info(f"  - Produtos: {dims.get('produto', 'N/A')}")
            self.logger.info(f"  - Tempo: {dims.get('tempo', 'N/A')}")
            self.logger.info(f"  - Fatos: {result.get('fact_records', 'N/A')}")
            
            self.logger.info(f"\nüîß Abordagem: {result.get('approach', 'N/A')}")
        else:
            self.logger.error(f"‚ùå Transforma√ß√£o falhou: {result.get('error', 'Erro desconhecido')}")

    def run_pipeline(self, stage: PipelineStage = PipelineStage.ALL) -> None:
        """
        Executa o pipeline ETL completo ou uma etapa espec√≠fica.
        
        Args:
            stage: Etapa do pipeline a ser executada. Padr√£o √© executar todas.
        """
        self.logger.info(f"\n=== Iniciando pipeline ETL - Etapa: {stage.value} ===")
        
        try:
            if stage in [PipelineStage.EXTRACT, PipelineStage.ALL]:
                self.extract()
            
            if stage in [PipelineStage.TRANSFORM, PipelineStage.ALL]:
                self.transform()
            
            if stage in [PipelineStage.LOAD, PipelineStage.ALL]:
                # TODO: Implementar etapa de load quando necess√°rio
                pass
            
            self.logger.info(f"\n=== Pipeline ETL conclu√≠do com sucesso - Etapa: {stage.value} ===")
            
        except Exception as e:
            self.logger.error(f"\n!!! Erro no pipeline ETL: {str(e)} !!!")
            raise

if __name__ == "__main__":
    # Exemplo de uso
    etl = FuelPriceETL()
    
    # Executar pipeline completo
    etl.run_pipeline()
    
    # Ou executar etapas espec√≠ficas
    # etl.run_pipeline(PipelineStage.EXTRACT)
    # etl.run_pipeline(PipelineStage.TRANSFORM)
